{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Baseline Model\n",
    "\n",
    "make models for each route(A, B, C, D) and prediction period(predict 2 weeks ahead, 4 weeks ahead, ...)\n",
    "\n",
    "## Dateset\n",
    "- [diff value from previous day](https://github.com/hiroshi-kuriyama/rail_level_gap/issues/2) of [rolling mean over days](https://github.com/hiroshi-kuriyama/rail_level_gap/issues/4)\n",
    "## Target Variable\n",
    "- vel_l values X weeks ahead (X = 2,4,6,8)\n",
    "## Features\n",
    "\n",
    "- track variables\n",
    "  - present lev_l value\n",
    "  - mean and variance of recent lev_l value\n",
    "  - mean and variance of recent days and whole year values of track data\n",
    "- equipment variables\n",
    "  - row value of equipment variables\n",
    "- seasonal variables\n",
    "  - date (encoded by trigonometric function)\n",
    "  - holiday dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "%matplotlib inline\n",
    "# from utils import data_process as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../input/'\n",
    "working_dir = '../working/'\n",
    "output_dir = '../output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeper_type_dict = {\n",
    "    1: 'pc',\n",
    "    2: 'wooden',\n",
    "    3: 'junction',\n",
    "    4: 'short',\n",
    "    5: 'synthetic',\n",
    "    6: 'synth_junc',\n",
    "    7: 'symth_short',\n",
    "    8: 'other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_track(line_name='a'):\n",
    "    track_df = pd.read_csv(os.path.join(input_dir, 'track_{}.csv'.format(line_name.upper())))\n",
    "    col_names_track = ['date', 'kilo', 'lev_l', 'lev_r', 'cur_l', 'cur_r', 'cant', 'width', 'speed']\n",
    "    track_df.columns = col_names_track\n",
    "    track_df['date'] = pd.to_datetime(track_df['date'])\n",
    "    print('track_{line_name} shape: {shape}'.format(line_name=line_name.upper(), shape=track_df.shape))\n",
    "    return track_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_equ(line_name='a'):\n",
    "    equ_df = pd.read_csv(os.path.join(input_dir, 'equipment_{}.csv'.format(line_name.upper())))\n",
    "    col_names_equ = ['kilo', 'is_ballast', 'is_long', 'sleeper_type', 'is_bridge', 'is_crossing', 'gross_ton', 'radius', 'is_unreliable']\n",
    "    equ_df.columns = col_names_equ\n",
    "    equ_df['sleeper_type'] = equ_df['sleeper_type'].replace(sleeper_type_dict).astype('category')\n",
    "    print('equ_{line_name} shape: {shape}'.format(line_name=line_name.upper(), shape=equ_df.shape))\n",
    "    return equ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrade data types to save memory\n",
    "def degrade_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype=='int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        if df[col].dtype=='float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_A shape: (10185690, 9)\n",
      "equ_A shape: (27906, 9)\n"
     ]
    }
   ],
   "source": [
    "track = read_track(abcd)\n",
    "track = degrade_dtypes(track)\n",
    "equ = read_equ(abcd)\n",
    "equ = degrade_dtypes(equ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submit = pd.read_csv(os.path.join(input_dir, 'sample_submit.csv'), header=None)\n",
    "index_master = pd.read_csv(os.path.join(input_dir, 'index_master.csv'))\n",
    "index_master.columns = ['id', 'line_name', 'date', 'kilo']\n",
    "index_master['date'] = pd.to_datetime(index_master['date'])\n",
    "index_master['kilo'] = index_master['kilo'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "### rolling average, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling average params\n",
    "roll_params = {\n",
    "    'A': {'window': 21, 'min_periods': 14},\n",
    "    'B': {'window': 14, 'min_periods': 7},\n",
    "    'C': {'window': 14, 'min_periods': 7},\n",
    "    'D': {'window': 14, 'min_periods': 7}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_diff(track, abcd):\n",
    "        # pivot: row is date, col is kilo\n",
    "        lev_pv = track.pivot(index='date', columns='kilo', values='lev_l')\n",
    "        lev_pv.columns = lev_pv.columns.astype('str')\n",
    "        # rolling average\n",
    "        lev_pv_ra = lev_pv.rolling(**roll_params[abcd.upper()], center=True, axis=0).mean()\n",
    "        # diff\n",
    "        lev_pv_ra_diff = lev_pv_ra.diff()\n",
    "        # reverse pivot\n",
    "        lev_ra_diff = pd.melt(lev_pv_ra_diff.reset_index(), id_vars='date', value_name='lev_l_diff')  \n",
    "        return lev_ra_diff, lev_pv_ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_ra_diff, lev_pv_ra = roll_diff(track, abcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tgt(lev_ra_diff):\n",
    "    lev_ra_diff_tgt_tmp = lev_ra_diff.copy()\n",
    "    lev_ra_diff_tgt_tmp['date_tgt'] = lev_ra_diff_tgt_tmp['date']\n",
    "    lev_ra_diff_tgt_tmp['date'] = lev_ra_diff_tgt_tmp['date'] - datetime.timedelta(weeks=2)\n",
    "    lev_ra_diff_tgt = lev_ra_diff_tgt_tmp.rename(columns={'lev_l_diff': 'lev_l_diff_tgt'})\n",
    "    return lev_ra_diff_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_ra_diff_tgt = make_tgt(lev_ra_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_mean(track):\n",
    "    track_whole_mean = track.groupby('kilo').mean()\n",
    "    track_whole_mean.columns = [i + '_w_mean' for i in track_whole_mean.columns]\n",
    "    track_whole_mean = track_whole_mean.reset_index()\n",
    "    track_whole_mean['kilo'] = track_whole_mean['kilo'].astype(str)\n",
    "    return track_whole_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_whole_mean = whole_mean(track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# copy target df\n",
    "data_all_tmp = lev_ra_diff_tgt.copy()\n",
    "# merge features\n",
    "data_all_tmp = data_all_tmp.merge(lev_ra_diff, on=['date', 'kilo'])\n",
    "data_all_tmp = data_all_tmp.merge(track_whole_mean, on='kilo')\n",
    "\n",
    "data_all_tmp = data_all_tmp.dropna(how='any', axis=0, subset=['lev_l_diff_tgt', 'lev_l_diff'])\n",
    "\n",
    "data_all = data_all_tmp\n",
    "    \n",
    "# del lev_ra_diff_tgt, lev_ra_diff, track_whole_mean, data_all_tmp, track, equ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_split(data, tgt_col='lev_l_diff_tgt', index_cols=['date', 'date_tgt', 'kilo']):\n",
    "    data = data.set_index(index_cols)\n",
    "    y = data[tgt_col]\n",
    "    X = data.drop(tgt_col, axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "threshold_date = '2018-01-01'\n",
    "X_dev, y_dev = X_y_split(data_all.query('date<\"{}\"'.format(threshold_date)))\n",
    "X_val, y_val = X_y_split(data_all.query('date>\"{}\"'.format(threshold_date)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_params = {\n",
    "    'bagging_freq': hp.uniform('bagging_freq', 1, 3),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.6, 0.8),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.1, 0.3),\n",
    "    'min_child_samples': hp.uniform('min_child_samples', 30, 100),\n",
    "    'num_leaves': hp.uniform('num_leaves', 500, 600),\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'bagging_freq': 1,\n",
    " 'feature_fraction': 0.7391192820324445,\n",
    " 'learning_rate': 0.0987018445595439,\n",
    " 'min_child_samples': 77,\n",
    " 'num_leaves': 548}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'early_stopping_rounds':20,\n",
    "    'eval_set':[(X_val, y_val)],\n",
    "    'eval_metric': 'mean_absolute_error',\n",
    "    'verbose': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float to int\n",
    "tobe_int_params = ['num_leaves', 'min_child_samples', 'bagging_freq']\n",
    "def int_param_encoder(params):\n",
    "    for param in tobe_int_params:\n",
    "        if param in params:\n",
    "            params[param] = int(params[param])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evals_i = 0\n",
    "def objective(hyperopt_params):\n",
    "    # パラメータを適切な型(int)に変換\n",
    "    hyperopt_params = int_param_encoder(hyperopt_params)\n",
    "    # モデルのインスタンス化\n",
    "    model = lgb.LGBMRegressor(**hyperopt_params, objective='mean_absolute_error', n_estimators=1000, random_state=0)\n",
    "    # trainデータを使ってモデルの学習\n",
    "    model.fit(X_dev, y_dev, **fit_params)\n",
    "    # validationデータを使用して、ラベルの予測\n",
    "    y_val_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "    # 予測ラベルと正解ラベルを使用してMAEを計算\n",
    "    mae_score = mean_absolute_error(y_val, y_val_pred)\n",
    "    global num_evals_i\n",
    "    num_evals_i += 1\n",
    "    print('[{num_evals}] best_ite: {best_ite}\\tMAE: {mae_score}'.format(num_evals=str(num_evals_i).zfill(4), mae_score=mae_score, best_ite=model.best_iteration_))\n",
    "    return mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001] best_ite: 125\tMAE: 0.020161671322544275\n",
      "[0002] best_ite: 114\tMAE: 0.02016566654119522\n",
      "[0003] best_ite: 151\tMAE: 0.02015735760328224\n",
      "[0004] best_ite: 306\tMAE: 0.020150840215652367\n",
      "[0005] best_ite: 319\tMAE: 0.0201222134321849\n",
      "[0006] best_ite: 146\tMAE: 0.020140648371190447\n",
      "[0007] best_ite: 153\tMAE: 0.020169474501504185\n",
      "[0008] best_ite: 113\tMAE: 0.020202191193999113\n",
      "[0009] best_ite: 667\tMAE: 0.020135206992456942\n",
      "[0010] best_ite: 197\tMAE: 0.020166727734974146\n",
      "[0011] best_ite: 362\tMAE: 0.020153899694917793\n",
      "[0012] best_ite: 137\tMAE: 0.020194769147428918\n",
      "[0013] best_ite: 394\tMAE: 0.020063812250391437\n",
      "[0014] best_ite: 35\tMAE: 0.020209465528431416\n"
     ]
    }
   ],
   "source": [
    "# iterationする回数\n",
    "max_evals = 20\n",
    "# 試行の過程を記録するインスタンス\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    # 最小化する値を定義した関数\n",
    "    fn=objective,\n",
    "    # 探索するパラメータのdictもしくはlist\n",
    "    space=hyperopt_params,\n",
    "    # どのロジックを利用するか、基本的にはtpe.suggestでok\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials,\n",
    "    # 試行の過程を出力\n",
    "    verbose=-1,\n",
    "    rstate=np.random.RandomState(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_param_encoder(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_dev, y_dev, X_val, y_val\n",
    "# make submission\n",
    "best_params = int_param_encoder(best)\n",
    "X, y = X_y_split(data_all)\n",
    "model_fulldata = lgb.LGBMRegressor(**best_params, n_estimators=30, random_state=0)\n",
    "model_fulldata.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, 'lgbm_baseline_20181116_model_{}.pkl'.format(abcd.upper())), mode='wb') as f:\n",
    "    pickle.dump(model_fulldata, f)\n",
    "    \n",
    "# with open(os.path.join(output_dir, 'lgbm_baseline_20181116_model_{}.pkl'.format(abcd.upper())), mode='rb') as f:\n",
    "#     model_fulldata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for Submission\n",
    "1. make initial values using last date lev_l_diff of each kilo\n",
    "2. predict 2 weeks ahead lev_l values by the initial values\n",
    "3. predcit 4 weeks ahead lev_l values by the 2 weeks ahead lev_l values\n",
    "4. repeat ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make initial values for future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_lev_ra_diff_index = lev_ra_diff.dropna(subset=['lev_l_diff'])\n",
    "last_date_lev_ra_diff_index = last_date_lev_ra_diff_index.groupby('kilo')['date'].max()\n",
    "last_date_lev_ra_diff_index = last_date_lev_ra_diff_index.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make index having latest date of each kilo\n",
    "# if the kilo has no value through all the period, fill them by NULL\n",
    "def make_last_date_lev_ra_diff_index(lev_ra_diff):\n",
    "    # latest date values of each kilo\n",
    "    last_date_lev_ra_diff_index = lev_ra_diff.dropna(subset=['lev_l_diff'])\n",
    "    last_date_lev_ra_diff_index = last_date_lev_ra_diff_index.groupby('kilo')['date'].max()\n",
    "    last_date_lev_ra_diff_index = last_date_lev_ra_diff_index.reset_index()\n",
    "    # make full_kilo index to fill missing kilo by NUll\n",
    "    kilo_index_tmp = lev_ra_diff.groupby('kilo')['kilo'].count().rename('tmp')\n",
    "    kilo_index_tmp = kilo_index_tmp.reset_index()\n",
    "    # join to full_kilo index\n",
    "    last_date_lev_ra_diff_index = kilo_index_tmp.merge(last_date_lev_ra_diff_index, on='kilo', how='left')\n",
    "    # drop un\n",
    "    last_date_lev_ra_diff_index = last_date_lev_ra_diff_index.drop('tmp', axis=1)\n",
    "    return last_date_lev_ra_diff_index\n",
    "\n",
    "# maek initial 'lev_l_diff' values of each kilo\n",
    "# The values are from the lastest date values of each kilo\n",
    "def make_last_date_lev_ra_diff(lev_ra_diff, last_date_lev_ra_diff_index):\n",
    "    # merge values into index\n",
    "    last_date_lev_ra_diff = last_date_lev_ra_diff_index.merge(lev_ra_diff, on=['date', 'kilo'], how='left')\n",
    "    # reset date\n",
    "    last_date_lev_ra_diff['date'] = datetime.datetime.strptime('2018-04-01', '%Y-%m-%d')\n",
    "    # fill missing 'lev_l_diff' by the next value\n",
    "    last_date_lev_ra_diff = last_date_lev_ra_diff.fillna(method='ffill')\n",
    "    return last_date_lev_ra_diff[['date', 'kilo', 'lev_l_diff']] # change the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_lev_ra_diff_index = make_last_date_lev_ra_diff_index(lev_ra_diff)\n",
    "last_date_lev_ra_diff = make_last_date_lev_ra_diff(lev_ra_diff, last_date_lev_ra_diff_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat prediction and making datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y\n",
    "# y -> X_next_01\n",
    "# X_next_01 -> y_next_02\n",
    "# y_next_02-> X_next_02\n",
    "# X_next_02 -> y_next_03 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y value is next X value\n",
    "def make_data_all_f(lev_ra_diff_gb_kilo):\n",
    "    data_all_f = lev_ra_diff_gb_kilo.merge(track_whole_mean, on=['kilo'])\n",
    "    data_all_f['date_tgt'] = data_all_f['date'] + datetime.timedelta(weeks=2)\n",
    "    data_all_f['lev_l_diff_tgt'] = -1.0\n",
    "    return data_all_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y_f_pred(data_all_f, model):\n",
    "    X_f, y_f = X_y_split(data_all_f)\n",
    "    y_f_pred_np = model_fulldata.predict(X_f)\n",
    "    y_f_pred = pd.Series(data=y_f_pred_np, index=y_f.index, name=y_f.name)\n",
    "    y_f_pred = y_f_pred.reset_index(level=['date_tgt', 'kilo']).reset_index(drop=True)\n",
    "    y_f_pred = y_f_pred.rename(columns={'date_tgt': 'date', 'lev_l_diff_tgt': 'lev_l_diff'})\n",
    "    return y_f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f_pred_pool = last_date_lev_ra_diff\n",
    "y_f_pred = last_date_lev_ra_diff\n",
    "for i in range(7): # 2week * 7 > 91days\n",
    "    data_all_f = make_data_all_f(y_f_pred)\n",
    "    y_f_pred = make_y_f_pred(data_all_f, model_fulldata)\n",
    "    y_f_pred_pool = pd.concat([y_f_pred_pool, y_f_pred], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape for submission and Fill unpredicted values by neighborhood values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_master_abcd = index_master.query('line_name==\"{}\"'.format(abcd.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into submission index \n",
    "pred_diff_df = index_master_abcd.merge(y_f_pred_pool, how='left', on=['date', 'kilo'])\n",
    "# fill NULL by last values\n",
    "pred_diff_df_pv = pred_diff_df.pivot(index='date', columns='kilo', values='lev_l_diff')\n",
    "pred_diff_df_pv.columns = pred_diff_df_pv.columns.astype('str')\n",
    "pred_diff_df_pv = pred_diff_df_pv.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial lev_l values (* not lev_l_diff)\n",
    "last_date_lev_ra = lev_pv_ra.fillna(method='ffill').tail(1)\n",
    "last_date_lev_ra = last_date_lev_ra.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulate lev_l_diff on lev_l\n",
    "pred_diff_df_pv.iloc[0,] = (pred_diff_df_pv.head(1).values + last_date_lev_ra.values)\n",
    "pred_df_pv = pred_diff_df_pv.cumsum(axis=0)\n",
    "pred_df = pd.melt(pred_df_pv.reset_index(), id_vars='date', value_name='lev_l_pred')\n",
    "pred_df['line_name'] = abcd.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(os.path.join(output_dir, 'lgbm_baseline_20181116_{}.csv'.format(abcd.upper())), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['lev_l_pred'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track['lev_l'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
